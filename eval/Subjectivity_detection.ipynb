{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec8c50a-1809-4f67-afe4-d365aef17bad",
   "metadata": {},
   "source": [
    "## Evaluation de la présence de terms de subjectivité et leur puissance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4deba0-0d0e-411a-b0a8-6a7e0d8ed4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79380241-3ac1-4987-8f4c-de986b516947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [str(word) for word in nlp(str(text))]\n",
    "\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babfc18e-e15f-44c7-9c18-be8052237587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_eval(file_path):\n",
    "    \n",
    "    subj_referenecs = pd.read_csv(\"./subjectivity_clues.csv\")\n",
    "    df_to_eval = pd.read_csv(file_path)\n",
    "    summaries = list(df_to_eval[\"summary\"])\n",
    "    #summaries = list(df_to_eval[\"summ_3\"])\n",
    "    \n",
    "    n_weak_scores, weak_scores = [], []\n",
    "    n_strong_scores,strong_scores = [], []\n",
    "    \n",
    "    for i, summary in enumerate(summaries):\n",
    "        n_weak_score, weak_score = 0., 0.\n",
    "        n_strong_score, strong_score = 0., 0.\n",
    "        token_list = tokenize(summary)\n",
    "        for token in token_list:\n",
    "            df_temp = subj_referenecs[subj_referenecs[\"word\"]==token]\n",
    "            if len(df_temp) > 0:\n",
    "                type_ = most_frequent(list(df_temp[\"type\"]))\n",
    "                if type_ == \"weaksubj\":\n",
    "                    weak_score += 1\n",
    "                    n_weak_score += 1/len(token_list)\n",
    "                if type_ == \"strongsubj\":\n",
    "                    strong_score +=1\n",
    "                    n_strong_score += 1/len(token_list)\n",
    "        \n",
    "        weak_scores.append(weak_score)\n",
    "        n_weak_scores.append(n_weak_score)\n",
    "        strong_scores.append(strong_score)\n",
    "        n_strong_scores.append(n_strong_score)\n",
    "    \n",
    "    return weak_scores, n_weak_scores, strong_scores, n_strong_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81867858-c557-4466-b15a-8d29a7634942",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meansum = \"../baselines/meansum_results_new.csv\"\n",
    "path_gpt2 = \"../baselines/gpt2_summaries.csv\"\n",
    "path_reference = \"../baselines/reference_summaries.csv\"\n",
    "path_textrank = \"../baselines/textrank_summaries.csv\"\n",
    "our_path = \"../outputs/train_300epochs.baseHtilt_cosineHhatversHhat_HtiltmeanHtiltcontext_FULL.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e3b031-6704-4db2-9862-9b612e0d719d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './subjectivity_clues.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m weak_scores, n_weak_scores, strong_scores, n_strong_scores = \u001b[43msubj_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mour_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36msubj_eval\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msubj_eval\u001b[39m(file_path):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     subj_referenecs = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./subjectivity_clues.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     df_to_eval = pd.read_csv(file_path)\n\u001b[32m      5\u001b[39m     summaries = \u001b[38;5;28mlist\u001b[39m(df_to_eval[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\towhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\towhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\towhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\towhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\towhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './subjectivity_clues.csv'"
     ]
    }
   ],
   "source": [
    "weak_scores, n_weak_scores, strong_scores, n_strong_scores = subj_eval(our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007e010-7ebe-4bd5-8123-029cc1fc199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6473429951690823\n",
      "0.5072463768115942\n"
     ]
    }
   ],
   "source": [
    "print(sum(weak_scores)/len(weak_scores))\n",
    "print(sum(strong_scores)/len(strong_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc65e5-6ad7-42fb-9b44-e5be71b8b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016966197265221812\n",
      "0.007764534603548347\n"
     ]
    }
   ],
   "source": [
    "print(sum(n_weak_scores)/len(n_weak_scores))\n",
    "print(sum(n_strong_scores)/len(n_strong_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d54cb4-1dde-4f16-add7-a317cd450aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de document ou il y a plus de weak que de strong : 91\n",
      "nombre de document ou il y a plus de strong que de weak : 82\n",
      "nombre de document ou same : 34\n"
     ]
    }
   ],
   "source": [
    "count_weak = 0\n",
    "count_strong = 0\n",
    "count_neutre = 0\n",
    "for i,_ in enumerate(weak_scores):\n",
    "    if weak_scores[i] > strong_scores[i]:\n",
    "        count_weak += 1\n",
    "    elif weak_scores[i] < strong_scores[i]:\n",
    "        count_strong += 1\n",
    "    else:\n",
    "        count_neutre += 1\n",
    "print(\"nombre de document ou il y a plus de weak que de strong :\", count_weak)\n",
    "print(\"nombre de document ou il y a plus de strong que de weak :\", count_strong)\n",
    "print(\"nombre de document ou same :\", count_neutre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d1d34-b1ca-409c-af57-2447ec29dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de document ou il y a plus de weak que de strong : 91\n",
      "nombre de document ou il y a plus de strong que de weak : 82\n",
      "nombre de document ou same : 34\n"
     ]
    }
   ],
   "source": [
    "count_weak = 0\n",
    "count_strong = 0\n",
    "count_neutre = 0\n",
    "for i,_ in enumerate(n_weak_scores):\n",
    "    if n_weak_scores[i] > n_strong_scores[i]:\n",
    "        count_weak += 1\n",
    "    elif n_weak_scores[i] < n_strong_scores[i]:\n",
    "        count_strong += 1\n",
    "    else:\n",
    "        count_neutre += 1\n",
    "print(\"nombre de document ou il y a plus de weak que de strong :\", count_weak)\n",
    "print(\"nombre de document ou il y a plus de strong que de weak :\", count_strong)\n",
    "print(\"nombre de document ou same :\", count_neutre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad80563-471e-4116-b4dd-6e920098c2eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354f3f9-23fe-449a-9bc6-bdbfb66dc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e170383-69ee-4ff8-9a1d-a7924a5d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vader(text, filter_POS=['PUNCT', 'DET'], stopwords_n=[]):\n",
    "    list_ = []\n",
    "    for token in nlp(str(text)):\n",
    "        if token.text in stopwords_n:\n",
    "            continue\n",
    "        elif token.pos_ in filter_POS:\n",
    "            continue\n",
    "        else:\n",
    "            list_.append(token.text.lower())\n",
    "    return ' '.join(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec19cd-1374-4108-b8cf-16e67cc083b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(file_path):\n",
    "     \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "     \n",
    "    df = pd.read_csv(file_path)\n",
    "    summaries = list(df[\"summary\"])\n",
    "    #summaries = list(df[\"summ_2\"])\n",
    "    \n",
    "    neu_dim, pos_dim, neg_dim = 0, 0, 0\n",
    "    neg_comp, pos_comp, neu_comp= 0, 0, 0\n",
    "\n",
    "    for summary in summaries : \n",
    "        summary = tokenize_vader(summary, stopwords_n=stopwords_nltk)\n",
    "        sentiment_dict = sid_obj.polarity_scores(summary)\n",
    "        ########### 1 -- -Getting main dimension\n",
    "        neg_score = sentiment_dict['neg']\n",
    "        neu_score = sentiment_dict['neu']\n",
    "        pos_score = sentiment_dict['pos']\n",
    "\n",
    "        main_dim_sent = sentiment_dict.copy()\n",
    "        del main_dim_sent['compound']\n",
    "        dimension = (max(main_dim_sent, key=main_dim_sent.get))\n",
    "        if dimension == 'neg':\n",
    "            neg_dim += 1\n",
    "        if dimension == 'pos':\n",
    "            pos_dim += 1\n",
    "        if dimension == 'neu':\n",
    "            neu_dim += 1\n",
    "        ################## 2 -- Getting the compound sentiment of summary\n",
    "        if sentiment_dict['compound'] >= 0.6 :\n",
    "            pos_comp += 1\n",
    "        elif sentiment_dict['compound'] <= - 0.6 :\n",
    "            neg_comp += 1\n",
    "        else: \n",
    "            neu_comp += 1\n",
    "            \n",
    "    return neu_dim, pos_dim, neg_dim, neg_comp, pos_comp, neu_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7780f3-89a8-4bfa-9c29-382cc4e57104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 57, 0, 0, 96, 111)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores(our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bfa68-4807-4fc3-80a4-f533eaacfce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
