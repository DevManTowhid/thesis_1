{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Notebook\n",
    "\n",
    "This notebook allows for training and evaluating the neutral summarization models. It includes steps for loading data, training the language model or summarizer, generating summaries, and evaluating performance using ROUGE scores.\n",
    "\n",
    "## Prerequisites\n",
    "- Ensure the environment is set up with required packages (see environment.yml or requirements.txt).\n",
    "- Data should be preprocessed and available in data/processed/.\n",
    "- Models will be saved in outputs/models/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Steps to Run the Notebook\n",
    "\n",
    "1. **Update Conda**: Run the first code cell to update conda to the latest version.\n",
    "2. **Create and Check Conda Environment**: Run the second code cell to create the conda environment (text-summarizer). **Note:** You must select this environment as your Kernel in Jupyter after creating it.\n",
    "3. **Import Libraries**: Run the third code cell to import all necessary libraries and download NLTK data.\n",
    "4. **Load Configurations**: Run the fourth cell to load dataset and hyperparameter configurations. Customize parameters as needed (e.g., batch size, epochs, mode).\n",
    "5. **Build Dataloaders**: Run the fifth cell to build train, valid, and test dataloaders.\n",
    "6. **Initialize Procedure**: Run the sixth cell to set up the training procedure with TensorBoard if enabled.\n",
    "7. **Train the Model**: Run the seventh cell to train either the language model ('lm') or summarizer ('summ'). This may take time depending on epochs.\n",
    "8. **Generate Summaries**: Run the eighth cell to generate summaries on the test set and save them to CSV.\n",
    "9. **Evaluate**: Run the ninth cell to compute ROUGE scores against reference summaries.\n",
    "\n",
    "Ensure the data is preprocessed before running. The environment.yml handles all installations, including pip dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update conda to the latest version (recommended to avoid installation issues)\n",
    "# Uncomment and run the following line\n",
    "# !conda update -n base -c defaults conda\n",
    "\n",
    "print('Update conda if prompted, then proceed to create the environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Conda environment (text-summarizer)\n",
    "# This will install all dependencies from environment.yml\n",
    "# !conda env create -f ../environment.yml\n",
    "\n",
    "# NOTE: You cannot activate a conda environment for the *current* notebook using '!conda activate'.\n",
    "# You must restart the kernel and select 'text-summarizer' from the Jupyter Kernel dropdown menu.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "env_name = 'text-summarizer'\n",
    "\n",
    "# Check if environment exists\n",
    "result_list = subprocess.run(['conda', 'env', 'list'], capture_output=True, text=True)\n",
    "env_exists = env_name in result_list.stdout\n",
    "\n",
    "# Check if currently running in the correct environment\n",
    "current_env_path = sys.prefix\n",
    "is_correct_env = env_name in current_env_path\n",
    "\n",
    "if is_correct_env:\n",
    "    print(f'SUCCESS: Currently running in the \"{env_name}\" environment.')\n",
    "elif env_exists:\n",
    "    print(f'WARNING: Environment \"{env_name}\" exists but is NOT active. Please switch the Jupyter Kernel to \"{env_name}\".')\n",
    "else:\n",
    "    print(f'ERROR: Environment \"{env_name}\" does not exist. Please run \"!conda env create...\" above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Personal libraries\n",
    "from configs.config import DatasetConfig, HP\n",
    "from data.DataLoader import build_dataloader\n",
    "from utils.Errors import loss_estimation\n",
    "from Procedures import Procedure\n",
    "from model.lm import LanguageModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from configs.config import follow\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "ds_config = DatasetConfig()\n",
    "hp = HP()\n",
    "\n",
    "# Customize parameters here\n",
    "ds_config.batch_size = 32  # Adjust batch size\n",
    "hp.lm_epochs = 10  # Number of epochs for LM training\n",
    "hp.summarizer_epochs = 10  # Number of epochs for summarizer training\n",
    "\n",
    "# Model options\n",
    "mode = 'summ'  # 'lm' for language model, 'summ' for summarizer\n",
    "model_name = 'my_model'  # Name for saving the model\n",
    "lm_path = None  # Path to pretrained LM if training summarizer\n",
    "\n",
    "# Override data paths to correct relative paths from notebooks/ directory\n",
    "ds_config.train_data = '../data/processed/train.mini.csv'\n",
    "ds_config.valid_data = '../data/processed/20221204_amazon_reviews_valid.csv'\n",
    "ds_config.test_data = '../data/processed/20221204_amazon_reviews_test.csv'\n",
    "\n",
    "print('Configurations loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloaders\n",
    "def build_dataloaders(ds_config, dataset=\"train\", vocab=None):\n",
    "    if dataset == \"train\":\n",
    "        train_iter, vocab, _ = build_dataloader(\n",
    "            file_path=ds_config.train_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=None,\n",
    "            is_train=True,\n",
    "            shuffle_batch=True,\n",
    "            max_num_reviews=ds_config.max_num_reviews,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return train_iter, vocab\n",
    "    elif dataset == \"valid\":\n",
    "        valid_iter, _, _ = build_dataloader(\n",
    "            file_path=ds_config.valid_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=vocab,\n",
    "            is_train=False,\n",
    "            shuffle_batch=False,\n",
    "            max_num_reviews=ds_config.max_num_reviews,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return valid_iter\n",
    "    elif dataset == \"test\":\n",
    "        test_iter, _, test_references = build_dataloader(\n",
    "            file_path=ds_config.test_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=vocab,\n",
    "            is_train=False,\n",
    "            shuffle_batch=False,\n",
    "            max_num_reviews=15,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return test_iter, test_references\n",
    "\n",
    "# Load data\n",
    "train_iter, vocab = build_dataloaders(ds_config, dataset=\"train\")\n",
    "valid_iter = build_dataloaders(ds_config, dataset=\"valid\", vocab=vocab)\n",
    "test_iter, test_references = build_dataloaders(ds_config, dataset=\"test\", vocab=vocab)\n",
    "\n",
    "print('Dataloaders built successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer (optional)\n",
    "if follow[\"writer\"]:\n",
    "    comment = follow[\"Name\"]\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "else:\n",
    "    writer = None\n",
    "\n",
    "# Initialize procedure\n",
    "# FIXED: Changed 'train_ter' to 'train_iter'\n",
    "procedure = Procedure(hp, ds_config, vocab, writer=writer, train_iter=train_iter, valid_iter=valid_iter)\n",
    "\n",
    "print('Procedure initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if mode == 'lm':\n",
    "    print('Training Language Model...')\n",
    "    procedure.train_lm(model_name=model_name, tolerance=3, check_every=5)\n",
    "    print('Language Model training completed.')\n",
    "elif mode == 'summ':\n",
    "    print('Training Summarizer...')\n",
    "    procedure.train_summarizer(model_name=model_name, lm_path=lm_path, tolerance=3, check_every=5)\n",
    "    print('Summarizer training completed.')\n",
    "else:\n",
    "    print('Invalid mode. Choose lm or summ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "print('Generating summaries...')\n",
    "summaries = procedure.generate_summaries(itr=test_iter, model_name=model_name)\n",
    "\n",
    "# Save summaries\n",
    "def save_summaries(summaries, output_path, model_name):\n",
    "    df = []\n",
    "    for e in summaries:\n",
    "        prod_id = e[0]\n",
    "        summary = e[1][0]\n",
    "        df.append({\n",
    "            \"model_path\": f\"outputs/models/summ/{model_name}\",\n",
    "            \"model_name\": model_name,\n",
    "            \"prod_id\": prod_id, \n",
    "            \"summary\": summary\n",
    "        })\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "output_path = f'./outputs/summaries/{model_name}_summaries.csv'\n",
    "summary_df = save_summaries(summaries, output_path, model_name)\n",
    "print(f'Summaries saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with ROUGE scores\n",
    "def tokenize(sentence):\n",
    "    INVALID_POS = [\"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RP\", \"TO\", \"WDT\", \"WP\", \"WRB\"]\n",
    "    # Fixed regex string formatting\n",
    "    sentence = re.sub(f\"[{re.escape(string.punctuation)}\\\\\\â€¦]+\", \" \", sentence)\n",
    "    tokens = nltk.pos_tag(sentence.split())\n",
    "    tokens = [tok for (tok, pos) in tokens if tok.lower() not in stop_words and pos not in INVALID_POS]\n",
    "    return tokens\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "refs = pd.read_csv(\"./eval/reference_summaries.csv\")\n",
    "\n",
    "def get_scores(path, refs):\n",
    "    df = pd.read_csv(path)\n",
    "    df = pd.merge(df, refs, how=\"inner\", on=\"prod_id\")\n",
    "    df[\"summary\"] = df[\"summary\"].replace(np.nan, \"\")\n",
    "    \n",
    "    all_scores = []\n",
    "    for prod_id in df[\"prod_id\"].unique():\n",
    "        data = df.loc[df[\"prod_id\"] == prod_id]\n",
    "        gen_summ = data[\"summary\"].values[0]\n",
    "        ref_summ = [data[\"summ_1\"].values[0], data[\"summ_2\"].values[0], data[\"summ_3\"].values[0]]\n",
    "        \n",
    "        prod_scores = []\n",
    "        for rs in ref_summ:\n",
    "            rs = \" \".join(tokenize(rs))\n",
    "            gen_summ_ = \" \".join(tokenize(gen_summ))\n",
    "            scores = scorer.score(gen_summ_, rs)\n",
    "            prod_scores.append([\n",
    "                [scores[\"rouge1\"].precision, scores[\"rouge1\"].recall, scores[\"rouge1\"].fmeasure],\n",
    "                [scores[\"rouge2\"].precision, scores[\"rouge2\"].recall, scores[\"rouge2\"].fmeasure],\n",
    "                [scores[\"rougeL\"].precision, scores[\"rougeL\"].recall, scores[\"rougeL\"].fmeasure]\n",
    "            ])\n",
    "        \n",
    "        prod_score_arr = np.array(prod_scores)\n",
    "        all_scores.append(prod_score_arr.mean(0))\n",
    "    \n",
    "    all_scores = np.array(all_scores)\n",
    "    mean_scores = all_scores.mean(0)\n",
    "    \n",
    "    output = {\n",
    "        \"rouge1\": {\n",
    "            \"precision\": round(mean_scores[0][0], 5),\n",
    "            \"recall\": round(mean_scores[0][1], 5),\n",
    "            \"fscore\": round(mean_scores[0][2], 5)\n",
    "        },\n",
    "        \"rouge2\": {\n",
    "            \"precision\": round(mean_scores[1][0], 5),\n",
    "            \"recall\": round(mean_scores[1][1], 5),\n",
    "            \"fscore\": round(mean_scores[1][2], 5)\n",
    "        },\n",
    "        \"rougeL\": {\n",
    "            \"precision\": round(mean_scores[2][0], 5),\n",
    "            \"recall\": round(mean_scores[2][1], 5),\n",
    "            \"fscore\": round(mean_scores[2][2], 5)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Compute scores\n",
    "scores = get_scores(output_path, refs)\n",
    "print('ROUGE Scores:', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
