{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Notebook\n",
    "\n",
    "This notebook allows for training and evaluating the neutral summarization models. It includes steps for loading data, training the language model or summarizer, generating summaries, and evaluating performance using ROUGE scores.\n",
    "\n",
    "## Prerequisites\n",
    "- Ensure the environment is set up with required packages (see environment.yml or requirements.txt).\n",
    "- Data should be preprocessed and available in data/processed/.\n",
    "- Models will be saved in outputs/models/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Steps to Run the Notebook\n",
    "\n",
    "1. **Update Conda**: Run the first code cell to update conda to the latest version.\n",
    "2. **Create and Check Conda Environment**: Run the second code cell to create and activate the conda environment (text-summarizer), and verify if it is created and activated.\n",
    "3. **Import Libraries**: Run the third code cell to import all necessary libraries and download NLTK data.\n",
    "4. **Load Configurations**: Run the fourth cell to load dataset and hyperparameter configurations. Customize parameters as needed (e.g., batch size, epochs, mode).\n",
    "5. **Build Dataloaders**: Run the fifth cell to build train, valid, and test dataloaders.\n",
    "6. **Initialize Procedure**: Run the sixth cell to set up the training procedure with TensorBoard if enabled.\n",
    "7. **Train the Model**: Run the seventh cell to train either the language model ('lm') or summarizer ('summ'). This may take time depending on epochs.\n",
    "8. **Generate Summaries**: Run the eighth cell to generate summaries on the test set and save them to CSV.\n",
    "9. **Evaluate**: Run the ninth cell to compute ROUGE scores against reference summaries.\n",
    "\n",
    "Ensure the data is preprocessed before running. The environment.yml handles all installations, including pip dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update conda to the latest version (recommended to avoid installation issues)\n",
    "# Uncomment and run the following line\n",
    "# !conda update -n base -c defaults conda\n",
    "\n",
    "print('Update conda if prompted, then proceed to create the environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate Conda environment (text-summarizer)\n",
    "# This will install all dependencies from environment.yml, including pip packages\n",
    "!conda env create -f ../environment.yml\n",
    "!conda activate text-summarizer\n",
    "\n",
    "# Check if conda environment is created and activated\n",
    "import subprocess\n",
    "\n",
    "env_name = 'text-summarizer'\n",
    "\n",
    "# Check if environment exists using conda env list\n",
    "result_list = subprocess.run(['conda', 'env', 'list'], capture_output=True, text=True)\n",
    "env_exists = env_name in result_list.stdout\n",
    "\n",
    "# Check if activated using conda info --envs\n",
    "result_info = subprocess.run(['conda', 'info', '--envs'], capture_output=True, text=True)\n",
    "env_activated = any('*' in line and env_name in line for line in result_info.stdout.split('\n'))\n",
    "\n",
    "if env_exists and env_activated:\n",
    "    print(f'Conda environment {env_name} is created and activated.')\n",
    "elif env_exists:\n",
    "    print(f'Conda environment {env_name} is created but not activated. Please activate {env_name}.')\n",
    "else:\n",
    "    print(f'Conda environment {env_name} is not created. Please create it using the previous cell.')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Personal libraries\n",
    "from configs.config import DatasetConfig, HP\n",
    "from data.DataLoader import build_dataloader\n",
    "from utils.Errors import loss_estimation\n",
    "from Procedures import Procedure\n",
    "from model.lm import LanguageModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from configs.config import follow\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "ds_config = DatasetConfig()\n",
    "hp = HP()\n",
    "\n",
    "# Customize parameters here\n",
    "ds_config.batch_size = 32  # Adjust batch size\n",
    "hp.lm_epochs = 10  # Number of epochs for LM training\n",
    "hp.summarizer_epochs = 10  # Number of epochs for summarizer training\n",
    "\n",
    "# Model options\n",
    "mode = 'summ'  # 'lm' for language model, 'summ' for summarizer\n",
    "model_name = 'my_model'  # Name for saving the model\n",
    "lm_path = None  # Path to pretrained LM if training summarizer\n",
    "\n",
    "# Override data paths to correct relative paths from notebooks/ directory\n",
    "ds_config.train_data = '../data/processed/train.mini.csv'\n",
    "ds_config.valid_data = '../data/processed/20221204_amazon_reviews_valid.csv'\n",
    "ds_config.test_data = '../data/processed/20221204_amazon_reviews_test.csv'\n",
    "\n",
    "print('Configurations loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloaders\n",
    "def build_dataloaders(ds_config, dataset=\"train\", vocab=None):\n",
    "    if dataset == \"train\":\n",
    "        train_iter, vocab, _ = build_dataloader(\n",
    "            file_path=ds_config.train_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=None,\n",
    "            is_train=True,\n",
    "            shuffle_batch=True,\n",
    "            max_num_reviews=ds_config.max_num_reviews,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return train_iter, vocab\n",
    "    elif dataset == \"valid\":\n",
    "        valid_iter, _, _ = build_dataloader(\n",
    "            file_path=ds_config.valid_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=vocab,\n",
    "            is_train=False,\n",
    "            shuffle_batch=False,\n",
    "            max_num_reviews=ds_config.max_num_reviews,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return valid_iter\n",
    "    elif dataset == \"test\":\n",
    "        test_iter, _, test_references = build_dataloader(\n",
    "            file_path=ds_config.test_data, \n",
    "            vocab_size=ds_config.vocab_size,\n",
    "            vocab_min_freq=ds_config.min_freq,\n",
    "            vocab=vocab,\n",
    "            is_train=False,\n",
    "            shuffle_batch=False,\n",
    "            max_num_reviews=15,\n",
    "            refs_path=None,\n",
    "            max_len_rev=ds_config.max_len_rev,\n",
    "            pin_memory=ds_config.pin_memory,\n",
    "            num_workers=ds_config.workers,\n",
    "            batch_size=ds_config.batch_size,\n",
    "            preprocess=True,\n",
    "            device=ds_config.device\n",
    "        )\n",
    "        return test_iter, test_references\n",
    "\n",
    "# Load data\n",
    "train_iter, vocab = build_dataloaders(ds_config, dataset=\"train\")\n",
    "valid_iter = build_dataloaders(ds_config, dataset=\"valid\", vocab=vocab)\n",
    "test_iter, test_references = build_dataloaders(ds_config, dataset=\"test\", vocab=vocab)\n",
    "\n",
    "print('Dataloaders built successfully.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer (optional)\n",
    "if follow[\"writer\"]:\n",
    "    comment = follow[\"Name\"]\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "else:\n",
    "    writer = None\n",
    "\n",
    "# Initialize procedure\n",
    "procedure = Procedure(hp, ds_config, vocab, writer=writer, train_ter=train_iter, valid_iter=valid_iter)\n",
    "\n",
    "print('Procedure initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if mode == 'lm':\n",
    "    print('Training Language Model...')\n",
    "    procedure.train_lm(model_name=model_name, tolerance=3, check_every=5)\n",
    "    print('Language Model training completed.')\n",
    "elif mode == 'summ':\n",
    "    print('Training Summarizer...')\n",
    "    procedure.train_summarizer(model_name=model_name, lm_path=lm_path, tolerance=3, check_every=5)\n",
    "    print('Summarizer training completed.')\n",
    "else:\n",
    "    print('Invalid mode. Choose lm or summ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "print('Generating summaries...')\n",
    "summaries = procedure.generate_summaries(itr=test_iter, model_name=model_name)\n",
    "\n",
    "# Save summaries\n",
    "def save_summaries(summaries, output_path, model_name):\n",
    "    df = []\n",
    "    for e in summaries:\n",
    "        prod_id = e[0]\n",
    "        summary = e[1][0]\n",
    "        df.append({\n",
    "            \"model_path\": f\"outputs/models/summ/{model_name}\",\n",
    "            \"model_name\": model_name,\n",
    "            \"prod_id\": prod_id, \n",
    "            \"summary\": summary\n",
    "        })\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "output_path = f'./outputs/summaries/{model_name}_summaries.csv'\n",
    "summary_df = save_summaries(summaries, output_path, model_name)\n",
    "print(f'Summaries saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with ROUGE scores\n",
    "def tokenize(sentence):\n",
    "    INVALID_POS = [\"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RP\", \"TO\", \"WDT\", \"WP\", \"WRB\"]\n",
    "    sentence = re.sub(f\"[{re.escape(string.punctuation)}\\\\…]+\", \" \", sentence)\n",
    "    tokens = nltk.pos_tag(sentence.split())\n",
    "    tokens = [tok for (tok, pos) in tokens if tok.lower() not in stop_words and pos not in INVALID_POS]\n",
    "    return tokens\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "refs = pd.read_csv(\"./eval/reference_summaries.csv\")\n",
    "\n",
    "def get_scores(path, refs):\n",
    "    df = pd.read_csv(path)\n",
    "    df = pd.merge(df, refs, how=\"inner\", on=\"prod_id\")\n",
    "    df[\"summary\"] = df[\"summary\"].replace(np.nan, \"\")\n",
    "    \n",
    "    all_scores = []\n",
    "    for prod_id in df[\"prod_id\"].unique():\n",
    "        data = df.loc[df[\"prod_id\"] == prod_id]\n",
    "        gen_summ = data[\"summary\"].values[0]\n",
    "        ref_summ = [data[\"summ_1\"].values[0], data[\"summ_2\"].values[0], data[\"summ_3\"].values[0]]\n",
    "        \n",
    "        prod_scores = []\n",
    "        for rs in ref_summ:\n",
    "            rs = \" \".join(tokenize(rs))\n",
    "            gen_summ_ = \" \".join(tokenize(gen_summ))\n",
    "            scores = scorer.score(gen_summ_, rs)\n",
    "            prod_scores.append([\n",
    "                [scores[\"rouge1\"].precision, scores[\"rouge1\"].recall, scores[\"rouge1\"].fmeasure],\n",
    "                [scores[\"rouge2\"].precision, scores[\"rouge2\"].recall, scores[\"rouge2\"].fmeasure],\n",
    "                [scores[\"rougeL\"].precision, scores[\"rougeL\"].recall, scores[\"rougeL\"].fmeasure]\n",
    "            ])\n",
    "        \n",
    "        prod_score_arr = np.array(prod_scores)\n",
    "        all_scores.append(prod_score_arr.mean(0))\n",
    "    \n",
    "    all_scores = np.array(all_scores)\n",
    "    mean_scores = all_scores.mean(0)\n",
    "    \n",
    "    output = {\n",
    "        \"rouge1\": {\n",
    "            \"precision\": round(mean_scores[0][0], 5),\n",
    "            \"recall\": round(mean_scores[0][1], 5),\n",
    "            \"fscore\": round(mean_scores[0][2], 5)\n",
    "        },\n",
    "        \"rouge2\": {\n",
    "            \"precision\": round(mean_scores[1][0], 5),\n",
    "            \"recall\": round(mean_scores[1][1], 5),\n",
    "            \"fscore\": round(mean_scores[1][2], 5)\n",
    "        },\n",
    "        \"rougeL\": {\n",
    "            \"precision\": round(mean_scores[2][0], 5),\n",
    "            \"recall\": round(mean_scores[2][1], 5),\n",
    "            \"fscore\": round(mean_scores[2][2], 5)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Compute scores\n",
    "scores = get_scores(output_path, refs)\n",
    "print('ROUGE Scores:', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
